{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the required libraries\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we load the data using numpy and do some pre-processing\n",
    "X_data = np.load('/home/pzaspel/data/md17_X.npy')\n",
    "y_data = np.load('/home/pzaspel/data/md17_Y.npy')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_data)\n",
    "\n",
    "y = y_data - np.average(y_data, 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we initialize the cluster i.e set of machines\n",
    "# n_worker refers to no of workes or cores that we need\n",
    "cluster = LocalCluster(host=\"titlis.clamv.jacobs-university.de\",scheduler_port=0, dashboard_address=\"titlis.clamv.jacobs-university.de:0\", n_workers=1, threads_per_worker=1, memory_limit='500GB')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the main forward stagewise regression function\n",
    "# no of iterations refers to how many iterations we wanna run the regression \n",
    "\n",
    "def forward_stagewise(X_train, X_test, y_train, y_test, no_of_iterations):\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # we initialize the varibales we need\n",
    "    m, n =  X_train.shape\n",
    "    beta = np.zeros(shape=(n, 1))\n",
    "    residual = client.persist(y_train)\n",
    "    X_train_transpose = client.persist(X_train.transpose()) \n",
    "\n",
    "    # active set is null at the begining \n",
    "    active_variable = set()\n",
    "\n",
    "    # list to record validation error at each iteration\n",
    "    mse_test = []\n",
    "\n",
    "    # no_varibles keeps record of variables after each variables gets added to our model\n",
    "    # mainly for plotting \n",
    "    no_variables = []\n",
    "\n",
    "    # this tracks the number of varibles at each iteration\n",
    "    # mainly for plotting\n",
    "    variables_iterations = []\n",
    "\n",
    "    # iterations is to count no of iterations we have completed\n",
    "    # mainly for plotting\n",
    "    iterations = 1\n",
    "\n",
    "    # this tracks if number of variables changes or not at each iteration\n",
    "    prev_variables = 0\n",
    "\n",
    "    # patience is the stopping criteria for iterations\n",
    "    # if after patience level 50, none of the variable gets added to our model\n",
    "    # we break out of the loop\n",
    "    patience = 0\n",
    "\n",
    "    for j in range(no_of_iterations):\n",
    "\n",
    "        # corr matrix stores the corelation of our variables with residual\n",
    "        corr = client.persist(da.dot(X_train_transpose, residual))\n",
    "\n",
    "        # i is the index of variable mostly corelated with our residual\n",
    "        i = client.persist(da.argmax(da.fabs(corr)))\n",
    "        index = i.compute()\n",
    "\n",
    "        # we add that index to active variable list\n",
    "        active_variable.add(index)\n",
    "\n",
    "        # to keep track of the sign of variable that we just added\n",
    "        if corr[i] < 0:\n",
    "            sign = -1\n",
    "        else:\n",
    "            sign = 1\n",
    "\n",
    "        # increase the cofficient by some constant i.e. learning rate multiplied with sign\n",
    "        beta[index, 0] = beta[index, 0] + (0.05 * sign)\n",
    "\n",
    "        # we load beta as dask array for parallel computation\n",
    "        beta_in_dask = client.persist(da.from_array(beta))\n",
    "\n",
    "        # this is our train prediction at each iteration\n",
    "        y_hat = client.persist(da.dot(X_train, beta_in_dask))\n",
    "\n",
    "        # new residual\n",
    "        residual = client.persist(y_train - y_hat)\n",
    "\n",
    "        total_var = len(active_variable)\n",
    "        variables_iterations.append(total_var)\n",
    "\n",
    "        # if new variables gets added to our model, we calculate validation error\n",
    "        if (prev_variables != total_var):\n",
    "            y_pred = client.persist(da.dot(X_test, beta_in_dask))\n",
    "            test_residual = client.persist(y_test - y_pred)\n",
    "            mse_test.append((da.square(test_residual).mean()).compute())\n",
    "            prev_variables = total_var\n",
    "            no_variables.append(prev_variables)\n",
    "            patience = 0\n",
    "\n",
    "        # if no new variable gets added, we increase patience level by 1\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        # if total no of variables reaches the dimension of data or patience level reaches 50\n",
    "        # we break the loop\n",
    "        if (prev_variables == n or patience == 50):\n",
    "            break\n",
    "\n",
    "        # print total number of variables in active set in every 100 iterations\n",
    "        if (iterations % 100 == 0):\n",
    "            print(total_var)\n",
    "\n",
    "        iterations += 1\n",
    "\n",
    "    end = time.time()\n",
    "    execution_time = (end-start)\n",
    "\n",
    "    return execution_time, mse_test, no_variables, iterations, variables_iterations\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to make variable plot, set this to true\n",
    "make_varibale_plots = False\n",
    "\n",
    "# intialize list to plot execution time vs no of cores plot\n",
    "no_of_cores = []\n",
    "total_execution_times = []\n",
    "\n",
    "# set no of iterations as you want\n",
    "no_of_iterations = 50\n",
    "\n",
    "# to scale cluster, we go from 1 to 40\n",
    "for k in range(1, 41):\n",
    "    client.restart()\n",
    "    client.cluster.scale(k)\n",
    "    \n",
    "    # we load the data into the cluster\n",
    "    future1 = client.scatter(X_train)\n",
    "    future2 = client.scatter(X_test)\n",
    "    future3 = client.scatter(y_train)\n",
    "    future4 = client.scatter(y_test)\n",
    "    \n",
    "    # for just one worker, we do not chunk the data\n",
    "    # we retrive the data from cluster as the dask array\n",
    "    if (k == 1):\n",
    "        X_t = da.from_delayed(future1, shape=X_train.shape, dtype=X_train.dtype)\n",
    "        X_t = X_t.rechunk((695265, 3121))\n",
    "        X_t = X_t.persist()\n",
    "        client.rebalance(X_t)\n",
    "\n",
    "        X_te = da.from_delayed(future2, shape=X_test.shape, dtype=X_test.dtype)\n",
    "        X_te = X_te.rechunk((297972, 3121))\n",
    "        X_te = X_te.persist()\n",
    "        client.rebalance(X_te)\n",
    "\n",
    "    \n",
    "        y_t = da.from_delayed(future3, shape=y_train.shape, dtype=y_train.dtype)\n",
    "        y_t = y_t.rechunk((695265, 1))\n",
    "        y_t = y_t.persist()\n",
    "        client.rebalance(y_t)\n",
    "    \n",
    "        y_te = da.from_delayed(future4, shape=y_test.shape, dtype=y_test.dtype)\n",
    "        y_te = y_te.rechunk((297972, 1))\n",
    "        y_te = y_te.persist()\n",
    "        client.rebalance(y_te)\n",
    "    \n",
    "    # if there are more workers, chunk the data\n",
    "    # we retrive the data from cluster as the dask array\n",
    "    # then, chunk the data\n",
    "    # spread the chunks over the cluster\n",
    "    else:\n",
    "        \n",
    "        \n",
    "        X_t = da.from_delayed(future1, shape=X_train.shape, dtype=X_train.dtype)\n",
    "        X_t = X_t.rechunk((7000, 3121))\n",
    "        X_t = X_t.persist()\n",
    "        client.rebalance(X_t)\n",
    "\n",
    "        X_te = da.from_delayed(future2, shape=X_test.shape, dtype=X_test.dtype)\n",
    "        X_te = X_te.rechunk((7000, 3121))\n",
    "        X_te = X_te.persist()\n",
    "        client.rebalance(X_te)\n",
    "\n",
    "\n",
    "        y_t = da.from_delayed(future3, shape=y_train.shape, dtype=y_train.dtype)\n",
    "        y_t = y_t.rechunk((7000, 1))\n",
    "        y_t = y_t.persist()\n",
    "        client.rebalance(y_t)\n",
    "\n",
    "\n",
    "        y_te = da.from_delayed(future4, shape=y_test.shape, dtype=y_test.dtype)\n",
    "        y_te = y_te.rechunk((7000, 1))\n",
    "        y_te = y_te.persist()\n",
    "        client.rebalance(y_te)\n",
    "    \n",
    "    execution_time, mse_test, no_variables, iterations, variables_iterations = forward_stagewise(\n",
    "                                                                                              X_t, \n",
    "                                                                                              X_te, \n",
    "                                                                                              y_t, \n",
    "                                                                                              y_te, \n",
    "                                                                                              no_of_iterations)\n",
    "    if (make_varibale_plots == True):\n",
    "        plt.plot(no_variables, mse_test, '-b', label='Validation Error')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title('Validation Error vs No of Variables')\n",
    "        plt.xlabel('No of Variables')\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(list(range(1, iterations)), variables_iterations, color='orange')\n",
    "        plt.title('No of variables vs Iterations')\n",
    "        plt.xlabel('No of iterations')\n",
    "        plt.ylabel('No of variables')\n",
    "        plt.show()\n",
    "    \n",
    "    total_execution_times.append(execution_time)\n",
    "    no_of_cores.append(k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we plot no of cores vs speedup\n",
    "total_execution_times = np.array(total_execution_times)\n",
    "speed_up = total_execution_times[0] / total_execution_times\n",
    "plt.plot(no_of_cores, speed_up, color='orange', label='speedup')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Speedup vs No of Cores')\n",
    "plt.xlabel('No of Cores')\n",
    "plt.ylabel('Speedup')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
