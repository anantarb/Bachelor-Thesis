{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we load the data using numpy and do some pre-processing\n",
    "X_data = np.load('/home/pzaspel/data/md17_X.npy')\n",
    "y_data = np.load('/home/pzaspel/data/md17_Y.npy')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_data)\n",
    "\n",
    "y = y_data - np.average(y_data, 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we initialize the cluster i.e set of machines\n",
    "# n_worker refers to no of workes or cores that we need\n",
    "cluster = LocalCluster(host=\"titlis.clamv.jacobs-university.de\",scheduler_port=0, dashboard_address=\"titlis.clamv.jacobs-university.de:0\", n_workers=1,  threads_per_worker=1, memory_limit='800GB')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least angle regression starts from here\n",
    "def lars(X_train, X_test, y_train, y_test, no_of_iterations):\n",
    "    start = time.time()\n",
    "    \n",
    "    # we initialize the varibales we need\n",
    "    m, n = X_train.shape\n",
    "    beta = np.zeros(shape=(n, 1))\n",
    "    residual = client.persist(y_train)\n",
    "    y_hat = da.zeros(shape=(m, 1))\n",
    "    \n",
    "    # active variable is empty at first\n",
    "    active_variable = []\n",
    "    \n",
    "    # inactive variable is full at the beginning\n",
    "    inactive_variable = list(range(n))\n",
    "    \n",
    "    # sign keeps tracks of the sign of active variables\n",
    "    sign = np.zeros(shape=(n, ), dtype=np.int8)\n",
    "    \n",
    "    # to record the validation error\n",
    "    mse_test = []\n",
    "    \n",
    "    X_train_transpose = client.persist(X_train.transpose())\n",
    "    \n",
    "    iterations = 1\n",
    "    \n",
    "    for j in range(no_of_iterations):\n",
    "        \n",
    "        # we calculate the correlation vector\n",
    "        c = client.persist(da.dot(X_train_transpose, residual))\n",
    "        \n",
    "        # extract the index of maximum element in c that is not in the active set\n",
    "        i = inactive_variable[(da.argmax(da.fabs(c[inactive_variable, :]))).compute()]\n",
    "        \n",
    "        # add i to active variable list\n",
    "        active_variable.append(i)\n",
    "        \n",
    "        # it just prints active variable at 100, 200, 300th... iterations\n",
    "        if (iterations % 100 == 0):\n",
    "            print(len(active_variable))\n",
    "            \n",
    "        # we remove i from inactive variable list\n",
    "        inactive_variable.remove(i)\n",
    "        \n",
    "        # we calculate the sign of highly correlated variable\n",
    "        if c[i] < 0:\n",
    "            sign[i] = -1\n",
    "        else:\n",
    "            sign[i] = 1\n",
    "        \n",
    "        # extract the active variables data from X\n",
    "        X_A = client.persist(X_train[:, active_variable] * da.asarray(sign[active_variable]))\n",
    "        \n",
    "        \n",
    "        # from here everything as described in original LARS paper\n",
    "        # we do caluclations to calculate the bisector\n",
    "        G_A = client.persist(da.dot(X_A.transpose(), X_A))\n",
    "    \n",
    "        G_A_INV = client.persist(da.linalg.inv(G_A))\n",
    "        \n",
    "        # this is Inverse G dot by 1 where length of 1 is len of active variables\n",
    "        G_A_1S = client.persist(da.sum(G_A_INV, 1))\n",
    "    \n",
    "        A_A = client.persist(1 / client.persist(da.sqrt(da.sum(G_A_1S))))\n",
    "    \n",
    "        w_a = client.persist(A_A * G_A_1S)\n",
    "    \n",
    "        w_a = client.persist(w_a.reshape(-1, 1))\n",
    "        \n",
    "        # this is equal bisector\n",
    "        y_a = client.persist(da.dot(X_A, w_a))\n",
    "        \n",
    "        # from here every calculations are done to calculate paramerter solution i.e gamma\n",
    "        C = da.fabs(c[i]).compute()\n",
    "    \n",
    "        a = client.persist(da.dot(X_train_transpose, y_a))\n",
    "\n",
    "        gamma = None\n",
    "\n",
    "        if len(active_variable) < n:\n",
    "            \n",
    "            # this is the first component in equation 5 see thesis\n",
    "            first_comp = client.persist((C - c[inactive_variable, :]) / (A_A - a[inactive_variable, :]))\n",
    "            \n",
    "            # this is the second component in equation 5 see thesis\n",
    "            second_comp = client.persist((C + c[inactive_variable, :]) / (A_A + a[inactive_variable, :]))\n",
    "\n",
    "            first_comp.compute_chunk_sizes()\n",
    "            second_comp.compute_chunk_sizes()\n",
    "            \n",
    "            \n",
    "            # only positive over the elements is taken\n",
    "            positive_elements = client.persist(da.extract(first_comp > 0, first_comp))\n",
    "            snd_positive_elements = client.persist(da.extract(second_comp > 0, second_comp))\n",
    "            \n",
    "            # now we calculate the minimum over the positive elements\n",
    "            gamma = min((da.nanmin(positive_elements).compute(), da.nanmin(snd_positive_elements).compute()))\n",
    "        \n",
    "        # when every variable is in the active set the above calculations does not hold\n",
    "        else:\n",
    "            gamma = (C / A_A).compute()\n",
    "        \n",
    "        # this is the new train prediction\n",
    "        y_hat = client.persist(y_hat + gamma * y_a)\n",
    "        \n",
    "        # current residual\n",
    "        residual = client.persist(y_train - y_hat)\n",
    "        \n",
    "        # we calculate the coefficients\n",
    "        beta[active_variable, :] = beta[active_variable , :] + gamma * np.asarray(w_a) * sign[active_variable].reshape(-1, 1)\n",
    "        \n",
    "        # convert it to dask since above calculation is done in NumPy\n",
    "        beta_in_dask = client.persist(da.asarray(beta))\n",
    "        \n",
    "        # this is the validation set prediction\n",
    "        y_pred = client.persist(da.dot(X_test, beta_in_dask))\n",
    "        \n",
    "        # we calculate the validation error \n",
    "        test_residual = client.persist(y_test - y_pred)\n",
    "        mse_test.append((da.square(test_residual).mean()).compute())\n",
    "        iterations += 1\n",
    "        \n",
    "    end = time.time()\n",
    "    execution_time = end - start\n",
    "    return execution_time, mse_test, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to make variable plot, set this to true\n",
    "make_varibale_plots = False\n",
    "\n",
    "# intialize list to plot execution time vs no of cores plot\n",
    "no_of_cores = []\n",
    "total_execution_times = []\n",
    "\n",
    "# set no of iterations as you want\n",
    "no_of_iterations = 500\n",
    "\n",
    "# to scale cluster, we go from 1 to 40\n",
    "for k in range(1, 41):\n",
    "    client.restart()\n",
    "    client.cluster.scale(k)\n",
    "    \n",
    "    # we load the data into the cluster\n",
    "    future1 = client.scatter(X_train)\n",
    "    future2 = client.scatter(X_test)\n",
    "    future3 = client.scatter(y_train)\n",
    "    future4 = client.scatter(y_test)\n",
    "    \n",
    "    # for just one worker, we do not chunk the data\n",
    "    # we retrive the data from cluster as the dask array\n",
    "    if (k == 1):\n",
    "        X_t = da.from_delayed(future1, shape=X_train.shape, dtype=X_train.dtype)\n",
    "        X_t = X_t.rechunk((695265, 3121))\n",
    "        X_t = X_t.persist()\n",
    "        client.rebalance(X_t)\n",
    "\n",
    "        X_te = da.from_delayed(future2, shape=X_test.shape, dtype=X_test.dtype)\n",
    "        X_te = X_te.rechunk((297972, 3121))\n",
    "        X_te = X_te.persist()\n",
    "        client.rebalance(X_te)\n",
    "\n",
    "    \n",
    "        y_t = da.from_delayed(future3, shape=y_train.shape, dtype=y_train.dtype)\n",
    "        y_t = y_t.rechunk((695265, 1))\n",
    "        y_t = y_t.persist()\n",
    "        client.rebalance(y_t)\n",
    "    \n",
    "        y_te = da.from_delayed(future4, shape=y_test.shape, dtype=y_test.dtype)\n",
    "        y_te = y_te.rechunk((297972, 1))\n",
    "        y_te = y_te.persist()\n",
    "        client.rebalance(y_te)\n",
    "    \n",
    "    # if there are more workers, chunk the data\n",
    "    # we retrive the data from cluster as the dask array\n",
    "    # then, chunk the data\n",
    "    # spread the chunks over the cluster\n",
    "    else:\n",
    "        \n",
    "\n",
    "        X_t = da.from_delayed(future1, shape=X_train.shape, dtype=X_train.dtype)\n",
    "        X_t = X_t.rechunk((200000, 3121))\n",
    "        X_t = X_t.persist()\n",
    "        client.rebalance(X_t)\n",
    "\n",
    "        X_te = da.from_delayed(future2, shape=X_test.shape, dtype=X_test.dtype)\n",
    "        X_te = X_te.rechunk((100000, 3121))\n",
    "        X_te = X_te.persist()\n",
    "        client.rebalance(X_te)\n",
    "\n",
    "\n",
    "        y_t = da.from_delayed(future3, shape=y_train.shape, dtype=y_train.dtype)\n",
    "        y_t = y_t.rechunk((695265, 1))\n",
    "        y_t = y_t.persist()\n",
    "        client.rebalance(y_t)\n",
    "\n",
    "\n",
    "        y_te = da.from_delayed(future4, shape=y_test.shape, dtype=y_test.dtype)\n",
    "        y_te = y_te.rechunk((297972, 1))\n",
    "        y_te = y_te.persist()\n",
    "        client.rebalance(y_te)\n",
    "    \n",
    "    execution_time, mse_test, iterations = lars(X_t, X_te, y_t, y_te, no_of_iterations)\n",
    "    \n",
    "    \n",
    "    if (make_varibale_plots == True):\n",
    "        plt.plot(list(range(1, iterations)), mse_test, '-b', label='Validation Error')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title('Validation Error vs No of Variables')\n",
    "        plt.xlabel('No of Variables')\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "        plt.show()\n",
    "    \n",
    "    total_execution_times.append(execution_time)\n",
    "    no_of_cores.append(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we plot no of cores vs speedup\n",
    "total_execution_times = np.array(total_execution_times)\n",
    "speed_up = total_execution_times[0] / total_execution_times\n",
    "plt.plot(no_of_cores, speed_up, color='orange', label='speedup')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Speedup vs No of Cores')\n",
    "plt.xlabel('No of Cores')\n",
    "plt.ylabel('Speedup')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
